{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis and Data Pre-processing\n",
    "\n",
    "#### In this notebook, the basic data understanding and data preparation is done as per second and third phase of CRISP-DM process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for EDA\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# For data Augmentation\n",
    "import random\n",
    "from scipy import ndarray\n",
    "import skimage as sk\n",
    "from skimage import transform\n",
    "from skimage import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the dataset path the cropped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"E:\\Class_Notes_Sem2\\ADM\\Project\\malaria-bounding-boxes\\malaria\\Cropped_images\"\n",
    "os.chdir(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIC EXPLORATORY DATA ANALYSIS\n",
    "* Get the counts for each of the category\n",
    "* Plot the barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the category and number of samples\n",
    "df = pd.DataFrame(columns = ['Path', 'Size'])\n",
    "for folders in os.listdir(dataset_path):\n",
    "    df.loc[folders.replace(' ','_')] = [os.path.join(dataset_path, folders)]+[len(os.listdir(folders))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df.index.values, height=df['Size'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset is heavily imbalanced\n",
    "\n",
    "### Techniques followed to handle data imbalance:\n",
    "* A threshold value of **2000** has been set to categorise the datasets for upsample and downsample.\n",
    "* For the dataset to be downsampled 2000 images has been chosen at random\n",
    "* For datasets to be upsampled, 2000 of total images are created for each of the dataset. Basic data augmentation techniques:\n",
    " * Random Rotation\n",
    " * Horizontal Flip\n",
    " * Random noise\n",
    " will be applied. `scikit-image` has been used as the library to do the augmentation. [Tutorial](https://medium.com/@thimblot/data-augmentation-boost-your-image-dataset-with-few-lines-of-python-155c2dc1baec)\n",
    "* There are total six classes:\n",
    " * gametocyte\n",
    " * red_blood_cell\n",
    " * ring\n",
    " * schizont\n",
    " * trophozoite\n",
    " * difficult\n",
    "\n",
    "* **difficult** is the set of category which could not be classified by human annotators as mentioned in the datasource [description](https://data.broadinstitute.org/bbbc/BBBC041/), hence we will be ommitting `difficult` category from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the classes to be upsample and classes to be downsampled\n",
    "classes_to_downsample = df[df['Size']>2000].index.values\n",
    "classes_to_upsample = df[df['Size']<=2000].index.values\n",
    "print(\"Classes going to be downsampled {} and classes going to upsampled are {}\".format(classes_to_downsample, classes_to_upsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotation(image_array: ndarray):\n",
    "    \"\"\"Rotate an image at random in with +50 to -50\"\"\"\n",
    "    random_degree = random.uniform(-50, 50)\n",
    "    return sk.transform.rotate(image_array, random_degree)\n",
    "\n",
    "def random_noise(image_array: ndarray):\n",
    "    \"\"\"Add random noise to the image\"\"\"\n",
    "    return sk.util.random_noise(image_array)\n",
    "\n",
    "def horizontal_flip(image_array: ndarray):\n",
    "    \"\"\"Flip the image\"\"\"\n",
    "    return image_array[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path containing some images\n",
    "cropped_images_path = r'E:\\Class_Notes_Sem2\\ADM\\Project\\malaria-bounding-boxes\\malaria\\Cropped_images'\n",
    "# For every classes to upsample run the following set of operations\n",
    "for categories in classes_to_upsample:\n",
    "    # If the category is difficult ignore and continue to the next category\n",
    "    if categories == \"difficult\":\n",
    "        continue\n",
    "    # Genrate 2000 more images for each category\n",
    "    num_files_desired = 2000\n",
    "    # Generate full path for each category of the images\n",
    "    path = os.path.join(cropped_images_path, categories)\n",
    "    # loop on all files of the folder and build a list of files paths\n",
    "    images = [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    # Instantiate the files generated\n",
    "    num_generated_files = 0\n",
    "    while num_generated_files <= num_files_desired:\n",
    "        # random image from the folder\n",
    "        image_path = random.choice(images)\n",
    "        # read image as an two dimensional array of pixels\n",
    "        image_to_transform = sk.io.imread(image_path)\n",
    "        available_transformations = {\n",
    "            'rotate': random_rotation,\n",
    "            'noise': random_noise,\n",
    "            'horizontal_flip': horizontal_flip\n",
    "        }\n",
    "\n",
    "        # Random number of transformations will be applied to an image\n",
    "        num_transformations_to_apply = random.randint(1, len(available_transformations))\n",
    "        num_transformations = 0\n",
    "        transformed_image = None\n",
    "        # Apply transformations\n",
    "        while num_transformations <= num_transformations_to_apply:\n",
    "            key = random.choice(list(available_transformations))\n",
    "            transformed_image = available_transformations[key](image_to_transform)\n",
    "            num_transformations += 1\n",
    "            new_file_path = '%s/augmented_image_%s.jpg' % (path, num_generated_files)\n",
    "            # Save images to the folder\n",
    "            sk.io.imsave(new_file_path, transformed_image)\n",
    "            num_generated_files += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The final set of images will be in a different location `Processed Image`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder processed images\n",
    "try:\n",
    "    os.mkdir(\"../Processed_Images\")\n",
    "except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Create folders for each category inside the processed images\n",
    "for categories in df.index.values:\n",
    "    if categories == 'difficult':\n",
    "        continue\n",
    "    try:\n",
    "        os.mkdir(\"../Processed_Images/{}\".format(categories))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy 2000 images from each category to the `Processed_image` folder at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for categories in df.index.values:\n",
    "    # Getting the file names\n",
    "    if categories == 'difficult':\n",
    "        continue\n",
    "    files = os.listdir(df.loc[categories]['Path'])\n",
    "    # Random Sampling the file indexes\n",
    "    sample_idx = np.random.choice(len(files), size=2000, replace=False)\n",
    "    for index in sample_idx:\n",
    "        try:\n",
    "            shutil.copy(os.path.join(df.loc[categories]['Path'], files[index]), \"../Processed_Images/{}\".format(categories))\n",
    "        except Exception as e:\n",
    "            print(\"Error occured as {}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution after the data augmentation and integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(columns = ['Size'])\n",
    "for folders in os.listdir(\"../Processed_Images\"):\n",
    "    df2.loc[folders.replace(' ','_')] = [len(os.listdir(os.path.join(\"../Processed_Images\", folders)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df2.index.values, height=df2['Size'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "* We will divide the data into three different parts.\n",
    " * **`train`**: This will contain 75% of the data, this data will be exposed to the model every epoch for the model to learn.\n",
    " * **`valid`**: This will contain 10% of the data, this data will be used to evaluate model's performance every epoch on unseen data. This will help us validate if the model is overfitting.\n",
    " * **`test`** : This set data will contain the rest 15%, which will be used to validate the accuracy metrics of the model after the model is fully trained.\n",
    "* We are making these datasets inside the `Processed_Images` folder itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, test and valid datasets\n",
    "for dataset_splits in ['train', 'test', 'valid']:\n",
    "    try:\n",
    "        os.mkdir(\"../Processed_Images/{}\".format(dataset_splits))\n",
    "        # Create folders for each category\n",
    "        for categories in df2.index.values:\n",
    "            try:\n",
    "                os.mkdir(\"../Processed_Images/{}/{}\".format(dataset_splits, categories))\n",
    "                os.mkdir(\"../Processed_Images/{}/{}\".format(dataset_splits, categories))\n",
    "                os.mkdir(\"../Processed_Images/{}/{}\".format(dataset_splits, categories))\n",
    "            except Exception as e:\n",
    "                print(\"Error occured as {}\".format(e))\n",
    "    except Exception as e:\n",
    "                print(\"Error occured as {}\".format(e))\n",
    "# Walk through each of the category and copy data to respective dataset\n",
    "for folders in os.listdir(\"../Processed_Images\"):\n",
    "    if folders in ['test', 'train', 'valid']:\n",
    "        continue\n",
    "    files = os.listdir(os.path.join(\"../Processed_Images\", folders))\n",
    "\n",
    "    # Shuffle the indexes for random sampling\n",
    "    sample_idx = np.random.choice(len(files), size=int(len(files)), replace=False)\n",
    "\n",
    "    # Copy the 75% data to the train dataset\n",
    "    for file in files[:int(len(files)*.75)]:\n",
    "        try:\n",
    "            shutil.copy(os.path.join(\"../Processed_Images/{}\".format(folders), file), \n",
    "                        \"../Processed_Images/train/{}\".format(folders))\n",
    "        except Exception as e:\n",
    "            print(\"Error occured as {}\".format(e))\n",
    "    \n",
    "    # Copy the next 10% data to the valid dataset\n",
    "    for file in files[int(len(files)*.75):int(len(files)*.85)]:\n",
    "        try:\n",
    "            shutil.copy(os.path.join(\"../Processed_Images/{}\".format(folders), file), \n",
    "                        \"../Processed_Images/valid/{}\".format(folders))\n",
    "        except Exception as e:\n",
    "            print(\"Error occured as {}\".format(e))\n",
    "    \n",
    "    # Copy the next 15% data to the valid dataset\n",
    "    for file in files[int(len(files)*.85):]:\n",
    "        try:\n",
    "            shutil.copy(os.path.join(\"../Processed_Images/{}\".format(folders), file), \n",
    "                        \"../Processed_Images/test/{}\".format(folders))\n",
    "        except Exception as e:\n",
    "            print(\"Error occured as {}\".format(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution in train, valid and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(columns=df2.index.values)\n",
    "for folders in ['train', 'test', 'valid']:\n",
    "    df3.loc[folders] = [len(os.listdir(os.path.join(\"../Processed_Images\", folders, f))) for f in df2.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu_pytorch)",
   "language": "python",
   "name": "gpu_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
