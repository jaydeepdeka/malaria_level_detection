{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Malaria Level Detection classifier\n",
    "\n",
    "* This notebook implements a classifier using PyTorch to detect different stages of the malaria.\n",
    "* The dataset used for this project has been downloaded from [kaggle](https://www.kaggle.com/kmader/malaria-bounding-boxes). The dataset contains total 1364 images with (~80000) cells annotated by human researchers in different categories.\n",
    "* In each of the images, tens of blood smears are present. There are two JSON files in the dataset, which contains details about:\n",
    " * Image **path**\n",
    " * **shape** containing size of the image and number of channels\n",
    " * **objects** containing `lower left co-ordinates` and `upper right co-ordinates` of the the blood smears and `category` of the smear.\n",
    "* We have used Python to crop out each cell using the co-ordinates of the images and save it to the respective folders created for each category. The script `crop_utils.py` uses opencv, pandas and other libraries.\n",
    "* Exploratory Data Analysis and data preprocessing is done as the dataset is highly imbalanced. We have used up-sampling and down-sampling to bring the data disctribution in a desired ratio. The details and implementation is in `EDA_DataPreProcessing.ipynb`.\n",
    "* The processed dataset is divided into three different subsets, `train`, `valid` and `test`.\n",
    "\n",
    "### Classifier implementation\n",
    "* As the dataset size is relatively small we have used [transfer learning](https://towardsdatascience.com/what-is-transfer-learning-8b1a0fa42b4), where a pre-trained model is used and we have customized the classifier part of the model.\n",
    "* As the pre-trained model, for better feature extraction we have used the model saved from `Pretrained_model.ipynb`.\n",
    "* In building the model we have done:\n",
    " * Data Transformation: [torchvision.transforms](https://pytorch.org/docs/stable/torchvision/transforms.html) module has been used for augmenting data while training to `flip`, `rotate`, `jitteruing`, ` cropping` and `normalizing`. The transformations are divided for `train` and `test and valid` separately as `test and validation` doesn't need same set of transformation.\n",
    " * We are feeding the network the dataset each epoch in batches of 16 for faster convergence.\n",
    " * We have dynamically allocatted the `device` based on availability of CUDA.\n",
    " * The `feature` network parametrs are frozen with pre-trained values and gradient calculation is set to False.\n",
    " * The customized fully connected `classifier` network uses:\n",
    "  * a layer 1024 neurons, which takes input from the `feature` CNN network.\n",
    "  * We have used [`ReLU`](https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU) as our activation function.\n",
    "  * And a [`dropout`](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout) of 0.2 is used to turn off 20% of the neurons randomly while training reduce overfitting and make the model more robust for generalisation.\n",
    " * As a loss function [`CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss) has been used as we have multiple categories.\n",
    " * Stochastic Gradient Descent([SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)) is used as the optimizer of for the network to update the parameters per batches per epoch.\n",
    " * We are decaying the learning rate at a rate of 0.2 for each 5 epoch for smooth convergence to the optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torchvision\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "%matplotlib inline\n",
    "\n",
    "torch.cuda.current_device() # Work around for the Bug https://github.com/pytorch/pytorch/issues/20635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directory\n",
    "data_dir = r\"E:\\Class_Notes_Sem2\\ADM\\Project\\malaria-bounding-boxes\\malaria\\Processed_Images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the batches of data every epoch every batches while traning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(50),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(240),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(240),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "dataset = { x : datasets.ImageFolder(os.path.join(data_dir, x), transformations[x])\n",
    "               for x in ['train', 'test', 'valid']\n",
    "          }\n",
    "\n",
    "dataset_loaders = {x : torch.utils.data.DataLoader(dataset[x], batch_size=16,\n",
    "                        shuffle=True, num_workers=4) for x in ['train', 'test', 'valid']\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically allocating the device for computation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model location\n",
    "model = torch.load(r'E:\\Class_Notes_Sem2\\ADM\\Project\\malaria_level_detection\\first_model.pth')\n",
    "\n",
    "# Setting requires_grad to false to stop calculating gradients for all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Getting the number of features coming from the feature network to the classifier network\n",
    "num_ftrs = model.classifier[0].in_features\n",
    "\n",
    "# Customizing the classifier network and replacing the loaded one, require_grad will be True for these by default.\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 1024),  \n",
    "    nn.ReLU(), \n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(), \n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(512, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Loss Function definition\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer for back propagation\n",
    "optimizer_classifier = optim.SGD(model.classifier.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.2 every 5 epochs\n",
    "classifier_lr_scheduler = lr_scheduler.StepLR(optimizer_classifier, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Training function.\n",
    "* For train and valid we are turning on and off the dropout layers.\n",
    "* We will be saving the model weights as per best accuracy on validation set.\n",
    "* General accuracy of the model will be printed for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer_cl, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode to avoid dropout\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataset_loaders[phase]:\n",
    "                # Getting the inputs and labels\n",
    "                inputs, labels = data\n",
    "                # Loading the model to the device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Gradient parameters are zeroed for every calculation\n",
    "                optimizer_cl.zero_grad()\n",
    "\n",
    "                # Forward pass and find the loss\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass, optimize only if in the training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer_cl.step()\n",
    "\n",
    "                # Get the statistics of loss and accuracy for each batch\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Get the statistics of loss and accuracy for each epoch\n",
    "            epoch_loss = running_loss / len(dataset[phase])\n",
    "            epoch_acc = running_corrects.item() / len(dataset[phase])\n",
    "            if phase == 'train':\n",
    "                train_loss.append(epoch_loss)\n",
    "                train_acc.append(epoch_acc)\n",
    "            else:\n",
    "                valid_loss.append(epoch_loss)\n",
    "                valid_acc.append(epoch_acc)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Copy the model with best accuracy\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_loss, valid_loss, train_acc, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, datalaoder, criterion):\n",
    "    model.train(False)\n",
    "    running_loss, running_corrects = 0.0, 0.0\n",
    "    for data in datalaoder:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    test_loss = running_loss / len(dataset['test'])\n",
    "    test_acc = running_corrects.item() / len(dataset['test'])\n",
    "    print('Test Loss: {:.4f} Acc: {:.4f}'.format(test_loss, test_acc))\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model for 25 epochs\n",
    "model_ft, train_loss, valid_loss, train_acc, valid_acc = train_model(model, criterion, optimizer_classifier, classifier_lr_scheduler,\n",
    "                       num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "evaluate_model(model_ft, dataset_loaders['test'], criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label=\"Training loss\")\n",
    "plt.plot(train_acc, label = \"Training accuracy\")\n",
    "plt.plot(valid_loss, label = \"Validation loss\")\n",
    "plt.plot(valid_acc, label = \"Validation accuracy\")\n",
    "plt.legend(labels = [\"Training loss\", \"Training accuracy\", \"Validation loss\", \"Validation accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up CUDA Cached memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model for final evaluation and Confusion Matrix\n",
    "* Here we are saving the trained model locally.\n",
    "* Re-loading the model into a different object and re-evaluating the performance and making the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, r\"E:\\Class_Notes_Sem2\\ADM\\Project\\malaria_level_detection\\malaria_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = torch.load(r\"E:\\Class_Notes_Sem2\\ADM\\Project\\malaria_level_detection\\malaria_classifier.pth\")\n",
    "# Evaluate the loaded model on test data\n",
    "evaluate_model(trained_model, dataset_loaders['test'], criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the confusion matrix for the model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the confusion matrix\n",
    "nb_classes = 5\n",
    "\n",
    "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "pred_list = []\n",
    "label_list = []\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(dataset_loaders['test']):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        label_list.append(labels)\n",
    "        outputs = trained_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        pred_list.append(preds)\n",
    "        for p, t in zip(preds.view(-1), labels.view(-1),):\n",
    "                confusion_matrix[p.long(), t.long()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_accuracy = confusion_matrix.diag().sum()/len(dataset_loaders['test'].dataset)\n",
    "print(\"Overall Accuracy of the model : {}\".format(overall_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test'].class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_df = pd.DataFrame(columns = dataset['test'].class_to_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, rows in enumerate(confusion_matrix):\n",
    "    confusion_matrix_df.loc[list(dataset['test'].class_to_idx.keys())[i]] = rows.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list_ar = []\n",
    "label_list_ar = []\n",
    "for i in pred_list:\n",
    "    pred_list_ar = pred_list_ar + list(i.cpu().numpy())\n",
    "for i in label_list:\n",
    "    label_list_ar = label_list_ar + list(i.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(label_list_ar, pred_list_ar))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu_pytorch)",
   "language": "python",
   "name": "gpu_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
