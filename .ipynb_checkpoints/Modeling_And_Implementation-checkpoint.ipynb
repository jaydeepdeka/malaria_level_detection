{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Malaria Level Detection classifier\n",
    "\n",
    "* This notebook implements a classifier using PyTorch to detect different stages of the malaria.\n",
    "* The dataset used for this project has been downloaded from [kaggle](https://www.kaggle.com/kmader/malaria-bounding-boxes). The dataset contains total 1364 images with (~80000) cells annotated by human researchers in different categories.\n",
    "* In each of the images, tens of blood smears are present. There are two JSON files in the dataset, which contains details about:\n",
    " * Image **path**\n",
    " * **shape** containing size of the image and number of channels\n",
    " * **objects** containing `lower left co-ordinates` and `upper right co-ordinates` of the the blood smears and `category` of the smear.\n",
    "* We have used Python to crop out each cell using the co-ordinates of the images and save it to the respective folders created for each category. The script `crop_utils.py` uses opencv, pandas and other libraries.\n",
    "* Exploratory Data Analysis and data preprocessing is done as the dataset is highly imbalanced. We have used up-sampling and down-sampling to bring the data disctribution in a desired ratio. The details and implementation is in `EDA_DataPreProcessing.ipynb`.\n",
    "* The processed dataset is divided into three different subsets, `train`, `valid` and `test`.\n",
    "\n",
    "### Classifier implementation\n",
    "* As the dataset size is relatively small we have used [transfer learning](https://towardsdatascience.com/what-is-transfer-learning-8b1a0fa42b4), where a pre-trained model is used and we have customized the classifier part of the model.\n",
    "* As the pre-trained model, for better feature extraction we have used the model saved from `Pretrained_model.ipynb`.\n",
    "* In building the model we have done:\n",
    " * Data Transformation: [torchvision.transforms](https://pytorch.org/docs/stable/torchvision/transforms.html) module has been used for augmenting data while training to `flip`, `rotate`, `jitteruing`, ` cropping` and `normalizing`. The transformations are divided for `train` and `test and valid` separately as `test and validation` doesn't need same set of transformation.\n",
    " * We are feeding the network the dataset each epoch in batches of 16 for faster convergence.\n",
    " * We have dynamically allocatted the `device` based on availability of CUDA.\n",
    " * The `feature` network parametrs are frozen with pre-trained values and gradient calculation is set to False.\n",
    " * The customized fully connected `classifier` network uses:\n",
    "  * a layer 1024 neurons, which takes input from the `feature` CNN network.\n",
    "  * We have used [`ReLU`](https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU) as our activation function.\n",
    "  * And a [`dropout`](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout) of 0.2 is used to turn off 20% of the neurons randomly while training reduce overfitting and make the model more robust for generalisation.\n",
    " * As a loss function [`CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss) has been used as we have multiple categories.\n",
    " * Stochastic Gradient Descent([SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)) is used as the optimizer of for the network to update the parameters per batches per epoch.\n",
    " * We are decaying the learning rate at a rate of 0.2 for each 5 epoch for smooth convergence to the optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the required modules\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torchvision\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "# from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "torch.cuda.current_device() # Work around for the Bug https://github.com/pytorch/pytorch/issues/20635"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directory\n",
    "data_dir = r\"E:\\Class_Notes_Sem2\\ADM\\Project\\malaria-bounding-boxes\\malaria\\Processed_Images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the batches of data every epoch every batches while traning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(50),\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(240),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(240),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "dataset = { x : datasets.ImageFolder(os.path.join(data_dir, x), transformations[x])\n",
    "               for x in ['train', 'test', 'valid']\n",
    "          }\n",
    "\n",
    "dataset_loaders = {x : torch.utils.data.DataLoader(dataset[x], batch_size=16,\n",
    "                        shuffle=True, num_workers=4) for x in ['train', 'test', 'valid']\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Dynamically allocating the device for computation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model location\n",
    "model = torch.load(r'E:\\Class_Notes_Sem2\\ADM\\Project\\malaria_level_detection\\first_model.pth')\n",
    "# To load VGG models with pretrained parameters\n",
    "# model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Setting requires_grad to false to stop calculating gradients for all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Getting the number of features coming from the feature network to the classifier network\n",
    "num_ftrs = model.classifier[0].in_features\n",
    "\n",
    "# Customizing the classifier network and replacing the loaded one, require_grad will be True for these by default.\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 1024),  \n",
    "    nn.ReLU(), \n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(), \n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(512, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Loss Function definition\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer for back propagation\n",
    "optimizer_classifier = optim.SGD(model.classifier.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.2 every 5 epochs\n",
    "classifier_lr_scheduler = lr_scheduler.StepLR(optimizer_classifier, step_size=5, gamma=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Training function.\n",
    "* For train and valid we are turning on and off the dropout layers.\n",
    "* We will be saving the model weights as per best accuracy on validation set.\n",
    "* General accuracy of the model will be printed for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer_cl, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode to avoid dropout\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataset_loaders[phase]:\n",
    "                # Getting the inputs and labels\n",
    "                inputs, labels = data\n",
    "                # Loading the model to the device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Gradient parameters are zeroed for every calculation\n",
    "                optimizer_cl.zero_grad()\n",
    "\n",
    "                # Forward pass and find the loss\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass, optimize only if in the training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer_cl.step()\n",
    "\n",
    "                # Get the statistics of loss and accuracy for each batch\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Get the statistics of loss and accuracy for each epoch\n",
    "            epoch_loss = running_loss / len(dataset[phase])\n",
    "            epoch_acc = running_corrects.item() / len(dataset[phase])\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Copy the model with best accuracy\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, datalaoder, criterion):\n",
    "    model.train(False)\n",
    "    running_loss, running_corrects = 0.0, 0.0\n",
    "    for data in datalaoder:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    test_loss = running_loss / len(dataset['test'])\n",
    "    test_acc = running_corrects.item() / len(dataset['test'])\n",
    "    print('Test Loss: {:.4f} Acc: {:.4f}'.format(test_loss, test_acc))\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "----------\n",
      "train Loss: 1.2071 Acc: 0.4753\n",
      "valid Loss: 1.0916 Acc: 0.5450\n",
      "\n",
      "Epoch 2/25\n",
      "----------\n",
      "train Loss: 1.0771 Acc: 0.5475\n",
      "valid Loss: 0.9052 Acc: 0.6160\n",
      "\n",
      "Epoch 3/25\n",
      "----------\n",
      "train Loss: 1.0390 Acc: 0.5613\n",
      "valid Loss: 0.9546 Acc: 0.5970\n",
      "\n",
      "Epoch 4/25\n",
      "----------\n",
      "train Loss: 0.9973 Acc: 0.5856\n",
      "valid Loss: 0.9286 Acc: 0.6440\n",
      "\n",
      "Epoch 5/25\n",
      "----------\n",
      "train Loss: 0.9296 Acc: 0.6144\n",
      "valid Loss: 0.9004 Acc: 0.6580\n",
      "\n",
      "Epoch 6/25\n",
      "----------\n",
      "train Loss: 0.9091 Acc: 0.6296\n",
      "valid Loss: 0.9123 Acc: 0.6070\n",
      "\n",
      "Epoch 7/25\n",
      "----------\n",
      "train Loss: 0.8882 Acc: 0.6349\n",
      "valid Loss: 0.8504 Acc: 0.6450\n",
      "\n",
      "Epoch 8/25\n",
      "----------\n",
      "train Loss: 0.8790 Acc: 0.6451\n",
      "valid Loss: 0.8145 Acc: 0.6700\n",
      "\n",
      "Epoch 9/25\n",
      "----------\n",
      "train Loss: 0.8685 Acc: 0.6457\n",
      "valid Loss: 0.7909 Acc: 0.6910\n",
      "\n",
      "Epoch 10/25\n",
      "----------\n",
      "train Loss: 0.8734 Acc: 0.6495\n",
      "valid Loss: 0.7931 Acc: 0.6910\n",
      "\n",
      "Epoch 11/25\n",
      "----------\n",
      "train Loss: 0.8460 Acc: 0.6503\n",
      "valid Loss: 0.8070 Acc: 0.6690\n",
      "\n",
      "Epoch 12/25\n",
      "----------\n",
      "train Loss: 0.8529 Acc: 0.6531\n",
      "valid Loss: 0.8034 Acc: 0.6780\n",
      "\n",
      "Epoch 13/25\n",
      "----------\n",
      "train Loss: 0.8537 Acc: 0.6448\n",
      "valid Loss: 0.7914 Acc: 0.6930\n",
      "\n",
      "Epoch 14/25\n",
      "----------\n",
      "train Loss: 0.8395 Acc: 0.6545\n",
      "valid Loss: 0.7867 Acc: 0.6780\n",
      "\n",
      "Epoch 15/25\n",
      "----------\n",
      "train Loss: 0.8371 Acc: 0.6584\n",
      "valid Loss: 0.7828 Acc: 0.6850\n",
      "\n",
      "Epoch 16/25\n",
      "----------\n",
      "train Loss: 0.8391 Acc: 0.6619\n",
      "valid Loss: 0.7811 Acc: 0.6850\n",
      "\n",
      "Epoch 17/25\n",
      "----------\n",
      "train Loss: 0.8506 Acc: 0.6567\n",
      "valid Loss: 0.7838 Acc: 0.6830\n",
      "\n",
      "Epoch 18/25\n",
      "----------\n",
      "train Loss: 0.8343 Acc: 0.6620\n",
      "valid Loss: 0.7832 Acc: 0.6830\n",
      "\n",
      "Epoch 19/25\n",
      "----------\n",
      "train Loss: 0.8381 Acc: 0.6604\n",
      "valid Loss: 0.7861 Acc: 0.6820\n",
      "\n",
      "Epoch 20/25\n",
      "----------\n",
      "train Loss: 0.8227 Acc: 0.6607\n",
      "valid Loss: 0.7861 Acc: 0.6810\n",
      "\n",
      "Epoch 21/25\n",
      "----------\n",
      "train Loss: 0.8530 Acc: 0.6473\n",
      "valid Loss: 0.7856 Acc: 0.6830\n",
      "\n",
      "Epoch 22/25\n",
      "----------\n",
      "train Loss: 0.8408 Acc: 0.6599\n",
      "valid Loss: 0.7864 Acc: 0.6830\n",
      "\n",
      "Epoch 23/25\n",
      "----------\n",
      "train Loss: 0.8314 Acc: 0.6577\n",
      "valid Loss: 0.7845 Acc: 0.6820\n",
      "\n",
      "Epoch 24/25\n",
      "----------\n",
      "train Loss: 0.8456 Acc: 0.6580\n",
      "valid Loss: 0.7854 Acc: 0.6820\n",
      "\n",
      "Epoch 25/25\n",
      "----------\n",
      "train Loss: 0.8309 Acc: 0.6639\n",
      "valid Loss: 0.7853 Acc: 0.6820\n",
      "\n",
      "Training complete in 49m 14s\n",
      "Best val Acc: 0.693000\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 25 epochs\n",
    "model_ft = train_model(model, criterion, optimizer_classifier, classifier_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7463 Acc: 0.7007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7463246703147888, 0.7006666666666667)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "evaluate_model(model_ft, dataset_loaders['test'], criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up CUDA Cached memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the confusion matrix for the model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the confusion matrix\n",
    "nb_classes = 5\n",
    "\n",
    "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(dataset_loaders['test']):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_ft(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5033, 0.7933, 0.8633, 0.7967, 0.5467])\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix.diag()/confusion_matrix.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gametocyte': 0,\n",
       " 'red_blood_cell': 1,\n",
       " 'ring': 2,\n",
       " 'schizont': 3,\n",
       " 'trophozoite': 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'].class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_df = pd.DataFrame(columns = dataset['test'].class_to_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rows in enumerate(te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu_pytorch)",
   "language": "python",
   "name": "gpu_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
